{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture all the project stages in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(x):\n",
    "    \n",
    "    # Modify data types\n",
    "    temp = x.astype({\"month\": \"O\", \"wday\": \"O\"})             \n",
    "    \n",
    "    # Fill null values\n",
    "    temp.loc[x[\"event_name_1\"].isna(),\"event_name_1\"] = 'no_event'\n",
    "\n",
    "    def fill_mode(records):\n",
    "        '''''\n",
    "        Function that fills the null values of\n",
    "        the records with values with the mode of that same product.\n",
    "        Returns the same records but with no nulls.\n",
    "        '''''\n",
    "        mode = records[\"sell_price\"].mode()[0]\n",
    "        records.loc[records[\"sell_price\"].isna(), \"sell_price\"] = mode\n",
    "        return records\n",
    "\n",
    "    temp = temp.groupby('item_id').apply(fill_mode)\n",
    "      \n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variables(x):\n",
    "\n",
    "    # Intermittent demand\n",
    "\n",
    "    def stock_outage(sales, n):\n",
    "        '''''\n",
    "        Function that returns a binary variable, with a value of 1 after N days of\n",
    "        0 sales in a row. It takes as an input the sales variable and the number of days\n",
    "        N.\n",
    "        '''''\n",
    "        zero_sales = pd.Series(np.where(sales == 0, 1, 0))\n",
    "        num_zeros = zero_sales.rolling(n).sum()\n",
    "        stock_outage = np.where(num_zeros == n, 1, 0)\n",
    "\n",
    "        return stock_outage\n",
    "    \n",
    "    x = x.sort_values(by = ['store_id','item_id','date'])\n",
    "    x[\"stock_outage_3\"] = x.groupby([\"store_id\", \"item_id\"])[\"sales\"].transform(lambda x: stock_outage(x,3))\n",
    "    x[\"stock_outage_7\"] = x.groupby([\"store_id\", \"item_id\"])[\"sales\"].transform(lambda x: stock_outage(x,7))\n",
    "    x[\"stock_outage_15\"] = x.groupby([\"store_id\", \"item_id\"])[\"sales\"].transform(lambda x: stock_outage(x,15))\n",
    "\n",
    "\n",
    "    # Lags variables\n",
    "\n",
    "    def create_lags(x, variable, n_lags):\n",
    "        '''''\n",
    "        Function that returns a dataframe with a n_lags variables which receive the\n",
    "        name of the input variable + \"_lag_\". Its value is just the shift of n_lags\n",
    "        backwards.\n",
    "        '''''\n",
    "        lags = pd.DataFrame()\n",
    "\n",
    "        #create all lags\n",
    "        for i in range(1, n_lags+1):\n",
    "            lags[variable + \"_lag_\" + str(i)] = df[variable].shift(i)\n",
    "\n",
    "        return lags\n",
    "    \n",
    "    lags_sell_price_x = x.groupby([\"store_id\", \"item_id\"]).apply(lambda x: create_lags(df=x, variable=\"sell_price\", n_lags=7))\n",
    "    lags_stock_outage_3_x = x.groupby([\"store_id\", \"item_id\"]).apply(lambda x: create_lags(df=x, variable=\"stock_outage_3\", n_lags=1))\n",
    "    lags_stock_outage_7_x = x.groupby([\"store_id\", \"item_id\"]).apply(lambda x: create_lags(df=x, variable=\"stock_outage_7\", n_lags=1))\n",
    "    lags_stock_outage_15_x = x.groupby([\"store_id\", \"item_id\"]).apply(lambda x: create_lags(df=x, variable=\"stock_outage_15\", n_lags=1))\n",
    "    lags_sales_x = x.groupby([\"store_id\", \"item_id\"]).apply(lambda x: create_lags(df=x, variable=\"sales\", n_lags=15))\n",
    "\n",
    "\n",
    "    # Moving window variables\n",
    "\n",
    "    def moving_minimum(x, variable, n_days):\n",
    "        '''''\n",
    "        Function that returns a dataframe with a n_days-1 variables. \n",
    "        It checks the variable \"variable\" and takes the minimum of the last i values\n",
    "        for each record of the original dataset df.\n",
    "        '''''\n",
    "        mvgmin = pd.DataFrame()\n",
    "\n",
    "        for i in range(2, n_days+1):\n",
    "            mvgmin[variable + \"_mvgmin_\" + str(i)] = x[variable].shift(1).rolling(i).min()\n",
    "\n",
    "        return mvgmin\n",
    "    \n",
    "    def moving_average(x, variable, n_days):\n",
    "    \n",
    "        mvgmean = pd.DataFrame()\n",
    "\n",
    "        for i in range(2, n_days+1):\n",
    "            mvgmean[variable + \"_mvgmean_\" + str(i)] = x[variable].shift(1).rolling(i).mean()\n",
    "\n",
    "        return mvgmean\n",
    "    \n",
    "    def moving_maximum(x, variable, n_days):\n",
    "\n",
    "        mvgmax = pd.DataFrame()\n",
    "\n",
    "        for i in range(2, n_days+1):\n",
    "            mvgmax[variable + \"_mvgmax_\" + str(i)] = x[variable].shift(1).rolling(i).max()\n",
    "\n",
    "        return mvgmax\n",
    "    \n",
    "    moving_minimum_x = x.groupby([\"store_id\",\"item_id\"]).apply(lambda x: moving_minimum(df=x, variable=\"sales\", n_days = 15))\n",
    "    moving_average_x = x.groupby([\"store_id\",\"item_id\"]).apply(lambda x: moving_average(df=x, variable=\"sales\", n_days = 15))\n",
    "    moving_maximum_x = x.groupby([\"store_id\",\"item_id\"]).apply(lambda x: moving_maximum(df=x, variable=\"sales\", n_days = 15))\n",
    "\n",
    "\n",
    "    # Join all the dataframes\n",
    "\n",
    "    temp = pd.concat([lags_sell_price_x,\n",
    "                    lags_stock_outage_3_x,\n",
    "                    lags_stock_outage_7_x,\n",
    "                    lags_stock_outage_15_x,\n",
    "                    lags_sales_x,\n",
    "                    moving_minimum_x,\n",
    "                    moving_average_x,\n",
    "                    moving_maximum_x], axis=1)\n",
    "    \n",
    "    x_concat = pd.concat([x, temp], axis=1)\n",
    "\n",
    "    x_concat.dropna(inplace=True)\n",
    "\n",
    "    to_remove = [\"d\",\"wm_yr_wk\",\"sell_price\",\"stock_outage_3\",\"stock_outage_7\",\"stock_outage_15\"]\n",
    "    x_concat.drop(columns=to_remove, inplace=True)\n",
    "\n",
    "    # Create a single variable for the product-store feature\n",
    "    x_concat.insert(loc=0, column=\"product_store\", value=(x_concat[\"store_id\"] + \"_\" + x_concat[\"item_id\"]))\n",
    "    x_concat.drop(columns=[\"item_id\",\"store_id\"], inplace=True)\n",
    "\n",
    "    return x_concat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_variables(x, y=None, mode=\"training\"):\n",
    "    '''\n",
    "    It takes the features' dataframe x and target dataframe y (if training mode is required). \n",
    "    It returns the transformed features and the non-transformed ones in a single dataframe x.\n",
    "    '''\n",
    "    x.reset_index(inplace=True)\n",
    "\n",
    "    # CATEGORICAL VARIABLES TRANSFORMATION\n",
    "    name_ohe = \"ohe_retail.pickle\"\n",
    "    name_te = \"te_retail.pickle\"\n",
    "    path_ohe = \"../data/\" + name_ohe\n",
    "    path_te = \"../data/\" + name_te\n",
    "\n",
    "    #ONE HOT ENCODING (OHE)\n",
    "    var_ohe = [\"event_name_1\"] # This is the variable that goes through OHE\n",
    "    if mode == \"training\":\n",
    "        # If mode = training then fit_transform and save the transformed\n",
    "        # variable in a pickle file\n",
    "        ohe = OneHotEncoder(sparse = False, handle_unknown = \"ignore\")\n",
    "        ohe_x = ohe.fit_transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns= ohe.get_feature_names_out())\n",
    "        with open(path_ohe, mode=\"wb\") as file:\n",
    "            pickle.dump(ohe, file)\n",
    "    else:\n",
    "        # If not, it is assumed the model is already trained and the corresponding\n",
    "        # pickle file is read from the data folder (and only applies transform)\n",
    "        with open(path_ohe, mode=\"rb\") as file:\n",
    "            ohe = pickle.load(file)\n",
    "        ohe_x = ohe.transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns= ohe.get_feature_names_out())\n",
    "\n",
    "    #TARGET ENCODING (TE)\n",
    "    var_te = [\"month\",\"wday\",\"weekday\"] # These are the variables that need TE\n",
    "    if mode == \"training\":\n",
    "        # Make sure y has the same exact index as x\n",
    "        y.reset_index(inplace=True, drop=True)\n",
    "        y = y.loc[y.index.isin(x.index)]\n",
    "        # If mode = training then fit_transform and save the transformed\n",
    "        # variable in a pickle file\n",
    "        te = TargetEncoder(min_samples_leaf = 100, return_df = False)\n",
    "        te_x = te.fit_transform(x[var_te], y = y)\n",
    "        var_te = [variable + \"_te\" for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns=var_te)\n",
    "        with open(path_te, mode=\"wb\") as file:\n",
    "            pickle.dump(te, file)\n",
    "    else:\n",
    "        # If not, it is assumed the model is already trained and the corresponding\n",
    "        # pickle file is read from the data folder (and only applies transform)\n",
    "        with open(path_te, mode=\"rb\") as file:\n",
    "            te = pickle.load(file)\n",
    "        te_x = te.transform(x[var_te])\n",
    "        var_te = [variable + \"_te\" for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns=var_te)\n",
    "    \n",
    "    #REMOVE THE NON-TRANSFORMED VARIABLES AND JOIN ALL THE TRANSFORMED\n",
    "    #ONES TOGETHER WITH x\n",
    "    # Remove the non-transformed variables\n",
    "    x = x.drop(columns=[\"event_name_1\", \"month\", \"wday\", \"weekday\"])\n",
    "    # Join all the dataframes\n",
    "    x = pd.concat([x, ohe_x, te_x], axis=1).set_index(\"date\")\n",
    "\n",
    "    # Return\n",
    "    return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x,y):\n",
    "    '''\n",
    "    Only to be used during training.\n",
    "    '''\n",
    "    #REMOVE THE INDEX AND THE product_store VARIABLE\n",
    "    x.reset_index(drop = True,inplace = True)\n",
    "    x.drop(columns=\"product_store\",inplace = True)\n",
    "    \n",
    "    #MAKE SURE THAT y HAS THE SAME EXACT INDEX POSITION AS x\n",
    "    y = y.loc[y.index.isin(x.index)]\n",
    "    \n",
    "    #COPY THE FEATURE SELECTION PROCESS DEFINED IN NOTEBOOK 5\n",
    "    mutual_selector = mutual_info_regression(x,y)\n",
    "    n_variables = 67\n",
    "    ranking_mi = pd.DataFrame(mutual_selector, index = x.columns).reset_index()\n",
    "    ranking_mi.columns = [\"variable\",\"importance_mi\"]\n",
    "    ranking_mi = ranking_mi.sort_values(by = \"importance_mi\", ascending = False)\n",
    "    ranking_mi[\"ranking_mi\"] = np.arange(0,ranking_mi.shape[0])\n",
    "    variables_mi = ranking_mi.iloc[0:n_variables][\"variable\"]\n",
    "    x_mi = x[variables_mi].copy()\n",
    "\n",
    "    return (x_mi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(x_product, y):\n",
    "    '''\n",
    "    Modelling of each product individually.\n",
    "\n",
    "    Takes the x and y data of one product, tunes the parameters for that model\n",
    "    and returns the best model for that product.\n",
    "    '''\n",
    "\n",
    "    # Feature to be modelled (exclude store_id and item_id)\n",
    "    var_model = x_product.columns.to_list()[2:]\n",
    "\n",
    "    # The test_size should be similar to the time frame we want to predict when\n",
    "    # the model is finished\n",
    "    time_cv = TimeSeriesSplit(n_splits=3, test_size=8) \n",
    "\n",
    "    # Using the default option of LightGBM works very well in this case\n",
    "\n",
    "    pipe = Pipeline([(\"algorithm\", HistGradientBoostingRegressor())])\n",
    "\n",
    "    grid = [    {\"algorithm\": [HistGradientBoostingRegressor()]\n",
    "                #  \"algorithm__learning_rate\": [0.01, 0.025, 0.05, 0.1],\n",
    "                #  \"algorithm__max_iter\": [50, 100, 200],\n",
    "                #  \"algorithm__max_depth\": [5, 10, 20],\n",
    "                #  \"algorithm__min_samples_leaf\": [500],\n",
    "                #  \"algorithm__scoring\": [\"neg_mean_absolute_error\"],\n",
    "                #  \"algorithm__l2_regularization\": [0, 0.25, 0.5, 0.75, 1],\n",
    "                }\n",
    "                # More algorithms could be added here if we wanted to test more\n",
    "            ]\n",
    "    # Apply random search\n",
    "    random_search = RandomizedSearchCV(estimator = pipe,\n",
    "                                   param_distributions = grid, \n",
    "                                   n_iter = 1, \n",
    "                                   cv = time_cv, \n",
    "                                   scoring = \"neg_mean_absolute_error\", \n",
    "                                   verbose = 0,\n",
    "                                   n_jobs = -1)\n",
    "\n",
    "    model = random_search.fit(x_product[var_model], y)\n",
    "\n",
    "    # Get the best model from the random search\n",
    "    final_model = model.best_estimator_.fit(x_product[var_model],y)\n",
    "\n",
    "    # Returns the best model\n",
    "    return (final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(df):     \n",
    "    '''\n",
    "    It takes the training dataset and for each product the modelling() function is used to create\n",
    "    a list with all the models for all the products.\n",
    "\n",
    "    The training dataframe contains the features x after cleaning and transformation, as well as the target.\n",
    "\n",
    "    It saves the trained models in a pickle format.\n",
    "    '''\n",
    "\n",
    "    product_store_list = list(df[\"product_store\"].unique())\n",
    "\n",
    "    list_models = []\n",
    "\n",
    "    for i in product_store_list:\n",
    "        \n",
    "        # Rename for readiness\n",
    "        product = i\n",
    "        target = \"sales\"\n",
    "\n",
    "        x = df.loc[df[\"product_store\"] == product,:].copy().drop(columns=target).copy()\n",
    "        y = df.loc[df[\"product_store\"] == product, target].copy()\n",
    "\n",
    "        x = transform_variables(x, y)\n",
    "        x = feature_selection(x, y)\n",
    "\n",
    "        # Call modelling function\n",
    "        model = modelling(x, y)\n",
    "\n",
    "        # Add the model to the list_models\n",
    "        list_models.append((product, model))\n",
    "\n",
    "    # Save the trained models' list\n",
    "    name_models = \"list_models_retail.pickle\"\n",
    "    path_models = \"../data/\" + name_models\n",
    "    with open(path_models, mode=\"wb\") as file:\n",
    "        pickle.dump(list_models, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_forecast(df):\n",
    "    '''\n",
    "    This function makes the forecast for each product but only for 1 day.\n",
    "\n",
    "    It takes the new dataset for making predictions.\n",
    "\n",
    "    For each product it loads its corresponding model, selects the data for that product and\n",
    "    makes the predictions.\n",
    "\n",
    "    It returns the forecast of all products but only for 1 day after the new dataset data.\n",
    "    '''\n",
    "\n",
    "    # LOAD MODELS\n",
    "    name_models = \"list_models_retail.pickle\"\n",
    "    path_models = \"../data/\" + name_models\n",
    "    with open(path_models, mode=\"rb\") as file:\n",
    "        list_models = pickle.load(file)\n",
    "\n",
    "    predictions_df = pd.DataFrame(columns=[\"date\",\"product\",\"sales\",\"prediction\"])\n",
    "\n",
    "    for i in range(0, len(list_models)):\n",
    "\n",
    "        product = list_models[i][0]\n",
    "        model = list_models[i][1]\n",
    "        variables = model[0].feature_names_in_ # Each model will have different variables\n",
    "        target = \"sales\"\n",
    "\n",
    "        x = df.loc[df[\"product_store\"] == product,:].copy().drop(columns=target).copy()\n",
    "        y = df.loc[df[\"product_store\"] == product, target].copy()\n",
    "\n",
    "        date = df.reset_index().copy()\n",
    "        date = date.loc[date[\"product_store\"] == product,\"date\"].values\n",
    "\n",
    "        # Transform variables (execution mode)\n",
    "        x = transform_variables(x, mode=\"execution\")\n",
    "\n",
    "        # Select features\n",
    "        x = x[variables]\n",
    "\n",
    "        # Compute predictions\n",
    "        predictions = pd.DataFrame(data = {\"date\": date,\n",
    "                                           \"product\": product,\n",
    "                                           \"sales\": y,\n",
    "                                           \"prediction\": model.predict(x)})\n",
    "        predictions[\"prediction\"] = predictions[\"prediction\"].astype(\"int\")\n",
    "\n",
    "        predictions_df = pd.concat([predictions_df, predictions], axis=0)\n",
    "    \n",
    "    predictions_df = predictions_df.loc[predictions_df.index == predictions_df.index.min()]\n",
    "    return (predictions_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_forecast(x):\n",
    "    \n",
    "    '''\n",
    "    This function does recursive forecasting to predict sales for the next 8 days.\n",
    "    Esta función es la que aplica el forecast recursivo para predecir 8 días.\n",
    "    \n",
    "    It takes the new dataset for making predictions.\n",
    "\n",
    "    It must have a specific structure so the whole process can be carried out.\n",
    "\n",
    "    In order to be recursive:\n",
    "\n",
    "    * It is going to predict the sales for the FIRST day it has all the data (i.e. 15 days from the oldest day - necessary to create stock, lag and moving window variables)\n",
    "    * Once the forecast is done, the oldest day (record) is removed from the dataset.\n",
    "    * Then for the next iteration it is going to predict the following day.\n",
    "\n",
    "    For example:\n",
    "\n",
    "    If the oldest day in the dataset is 09.12.2015 then the first day we can predict (and we don't\n",
    "    have the data) is 24.12.2015. When sales are predicted for that day, then the record for that\n",
    "    day is overwritten with the actual prediction, and the record for 09.12.2015 is removed.\n",
    "    Then, the oldest day now is 10.12.2015 and the new prediction is going to be for 25.12.2015.\n",
    "    '''\n",
    "    \n",
    "    for cada in range(0,8): \n",
    "        paso1_df = cleaning_data(x)\n",
    "        paso2_df = create_variables(paso1_df)\n",
    "        \n",
    "        #Calcula la predicción\n",
    "        f = start_forecast(paso2_df)\n",
    "        f['store_id'] = f[\"product_store\"].str[:4]\n",
    "        f['item_id'] = f[\"product_store\"].str[5:]\n",
    "\n",
    "        #Actualiza el dato de ventas con la predicción\n",
    "        x.loc[(x.index.isin(f.date)) & (x.store_id.isin(f.store_id)) & (x.item_id.isin(f.item_id)),'sales'] = f.prediction\n",
    "                                                              \n",
    "        #Elimina el día más antiguo de x\n",
    "        x = x.loc[x.index != x.index.min()]\n",
    "        \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
